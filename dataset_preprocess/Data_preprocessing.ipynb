{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735729cc",
   "metadata": {},
   "source": [
    "CGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b70d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import zhconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2057e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs_cged2014(file_name):\n",
    "    logging.info((\"Reading lines from {}\".format(file_name)))\n",
    "    total_correct=[]\n",
    "    total_mistakes=[]\n",
    "    with codecs.open(file_name, \"r\", \"utf-8\") as file:\n",
    " \n",
    "        data = file.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        results = soup.find_all('essay')\n",
    "        for item in tqdm(results):\n",
    "            text = item.find(\"text\").text.strip()\n",
    "            mistakes=text.split('\\n')\n",
    "            correct_text = [x.text.strip() for x in item.find_all(\"correction\")]\n",
    "            total_correct=total_correct+correct_text\n",
    "            total_mistakes=total_mistakes+mistakes\n",
    "    assert len(total_correct)==len(total_mistakes)\n",
    "    return total_correct,total_mistakes\n",
    "def rewrite_cged2014(file_name):\n",
    "    total_correct,total_mistakes=read_langs_cged2014(file_name+\".sgml\")\n",
    "    with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "        fp.close()\n",
    "    with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc399d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 388/388 [00:00<00:00, 43108.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 232/232 [00:00<00:00, 33140.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2014/nlptea14cfl_release1.1/Training/A2_CFL_training'\n",
    "rewrite_cged2014(file_name)\n",
    "# file_name='./CGED/cged2014/nlptea14cfl_release1.1/Training/B1_CFL_training'\n",
    "# rewrite_cged2014(file_name)\n",
    "# 存在编码问题\n",
    "file_name='./CGED/cged2014/nlptea14cfl_release1.1/Training/B2_CFL_training'\n",
    "rewrite_cged2014(file_name)\n",
    "file_name='./CGED/cged2014/nlptea14cfl_release1.1/Training/C1_CFL_training'\n",
    "rewrite_cged2014(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f20142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs_cged2015(file_name):\n",
    "    logging.info((\"Reading lines from {}\".format(file_name)))\n",
    "    total_correct=[]\n",
    "    total_mistakes=[]\n",
    "    with codecs.open(file_name, \"r\", \"utf-8\") as file:\n",
    " \n",
    "        data = file.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        results = soup.find_all('doc')\n",
    "\n",
    "        for item in tqdm(results):\n",
    "            text = item.find(\"sentence\").text.strip()\n",
    "            mistakes=text.split('\\n')\n",
    "            correct_text = [x.text.strip() for x in item.find_all(\"correction\")]\n",
    "            total_correct=total_correct+correct_text\n",
    "            total_mistakes=total_mistakes+mistakes\n",
    "    assert len(total_correct)==len(total_mistakes)\n",
    "    return total_correct,total_mistakes\n",
    "def rewrite_cged2015(file_name):\n",
    "    total_correct,total_mistakes=read_langs_cged2015(file_name+\".sgml\")\n",
    "    with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "        fp.close()\n",
    "    with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7da6dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2205/2205 [00:00<00:00, 55755.82it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2015/nlptea15cged_release1.0/Training/NLPTEA15_CGED_Training'\n",
    "rewrite_cged2015(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32b1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs_cged2016(file_name):\n",
    "    logging.info((\"Reading lines from {}\".format(file_name)))\n",
    "    total_correct=[]\n",
    "    total_mistakes=[]\n",
    "    with codecs.open(file_name, \"r\", \"utf-8\") as file:\n",
    " \n",
    "        data = file.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        results = soup.find_all('doc')\n",
    "\n",
    "        for item in tqdm(results):\n",
    "            text = item.find(\"text\").text.strip()\n",
    "            mistakes=text.split('\\n')\n",
    "            correct_text = [x.text.strip() for x in item.find_all(\"correction\")]\n",
    "            total_correct=total_correct+correct_text\n",
    "            total_mistakes=total_mistakes+mistakes\n",
    "    assert len(total_correct)==len(total_mistakes)\n",
    "    return total_correct,total_mistakes\n",
    "def rewrite_cged2016(file_name):\n",
    "    total_correct,total_mistakes=read_langs_cged2016(file_name+\".txt\")\n",
    "    with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "        fp.close()\n",
    "    with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99904aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9602/9602 [00:00<00:00, 19258.88it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2016/nlptea16cged_release1.0/Training/CGED16_HSK_TrainingSet'\n",
    "rewrite_cged2016(file_name)\n",
    "# file_name='./CGED/cged2016/nlptea16cged_release1.0/Training/CGED16_TOCFL_TrainingSet'\n",
    "# rewrite_cged2016(file_name)\n",
    "# 存在编码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e60668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs_cged2017(file_name):\n",
    "    logging.info((\"Reading lines from {}\".format(file_name)))\n",
    "    total_correct=[]\n",
    "    total_mistakes=[]\n",
    "    with codecs.open(file_name, \"r\", \"utf-8\") as file:\n",
    " \n",
    "        data = file.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        results = soup.find_all('doc')\n",
    "\n",
    "        for item in tqdm(results):\n",
    "            if item.find(\"text\") is None:\n",
    "                continue\n",
    "            text = item.find(\"text\").text.strip()\n",
    "            mistakes=text.split('\\n')\n",
    "            correct_text = [x.text.strip() for x in item.find_all(\"correction\")]\n",
    "            total_correct=total_correct+correct_text\n",
    "            total_mistakes=total_mistakes+mistakes\n",
    "    assert len(total_correct)==len(total_mistakes)\n",
    "    return total_correct,total_mistakes\n",
    "def rewrite_cged2017(file_name):\n",
    "    total_correct,total_mistakes=read_langs_cged2017(file_name+\".xml\")\n",
    "    with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "        fp.close()\n",
    "    with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f747ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10449/10449 [00:00<00:00, 15375.26it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2017/train.release'\n",
    "rewrite_cged2017(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b93890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 402/402 [00:00<00:00, 50246.15it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2018/train_CGED2018'\n",
    "rewrite_cged2017(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf05e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1156/1156 [00:00<00:00, 48165.85it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2020/test_truth_2020mk'\n",
    "rewrite_cged2017(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9234d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2282/2282 [00:00<00:00, 45181.37it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./CGED/cged2021/test_2021'\n",
    "rewrite_cged2017(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd90c8f",
   "metadata": {},
   "source": [
    "CTC2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ee20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "CTC2021 = []\n",
    "file_name='./CTC2021/train/train_large_v2'\n",
    "for line in open('./CTC2021/train/train_large_v2.json','r',encoding='utf8'): \n",
    "    CTC2021.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18aeb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "for item in CTC2021:\n",
    "    total_correct.append(item['target'])\n",
    "    total_mistakes.append(item['source'])\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b8d0",
   "metadata": {},
   "source": [
    "Lang8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "926c4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./Lang8/data.train'\n",
    "for line in open('./Lang8/data.train','r',encoding='utf8'): \n",
    "    data=line[:-1].split('\\t')\n",
    "    if len(data)!=int(data[1])+3:\n",
    "        continue\n",
    "    if int(data[1])==0:\n",
    "        total_mistakes.append(data[2])\n",
    "        total_correct.append(data[2])\n",
    "    else:\n",
    "        for i in range(int(data[1])):\n",
    "            total_mistakes.append(data[2])\n",
    "            total_correct.append(data[3+i])\n",
    "\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70debe94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1220098"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab840465",
   "metadata": {},
   "source": [
    "MuCGEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a956803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./MuCGEC/MuCGEC_dev'\n",
    "for line in open(file_name+\".txt\",'r',encoding='utf8'): \n",
    "    data=line[:-1].split('\\t')\n",
    "    if data[2]==\"没有错误\":\n",
    "        total_mistakes.append(data[1])\n",
    "        total_correct.append(data[1])\n",
    "    else:\n",
    "        for i in range(len(data)-2):\n",
    "            total_mistakes.append(data[1])\n",
    "            total_correct.append(data[2+i])\n",
    "\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c12007",
   "metadata": {},
   "source": [
    "MuCGEC_Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45ff225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./MuCGEC_exp_data/train/train'\n",
    "for line in open(file_name+\".para\",'r',encoding='utf8'): \n",
    "    data=line[:-1].split('\\t')\n",
    "    total_mistakes.append(data[0])\n",
    "    total_correct.append(data[1])\n",
    "    \n",
    "\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "315647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./MuCGEC_exp_data/valid/valid'\n",
    "for line in open(file_name+\".para\",'r',encoding='utf8'): \n",
    "    data=line[:-1].split('\\t')\n",
    "    total_mistakes.append(data[0])\n",
    "    total_correct.append(data[1])\n",
    "\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e1ca1",
   "metadata": {},
   "source": [
    "YACLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "730b9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "YACLC = []\n",
    "file_name='./YACLC/valid'\n",
    "for line in open(file_name+'.jsonl','r',encoding='utf8'): \n",
    "    YACLC.append(json.loads(line))\n",
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "for item in YACLC:\n",
    "    for correct in item['sentence_annos']:\n",
    "        total_mistakes.append(item['sentence_text'])\n",
    "        total_correct.append(correct['correction'])\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18ff571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./CSC_val/val'\n",
    "for line in open(\"./CSC_val/data.txt\",'r',encoding='utf8'): \n",
    "    data=line[:-1].split('\\t')\n",
    "    total_mistakes.append(data[0])\n",
    "    total_correct.append(data[1])\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "362a0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "file_name='./CSC_test/test'\n",
    "for line in open(\"./CSC_test/NLPCC_TASK8_TESTDATA.txt\",'r',encoding='utf8'): \n",
    "    total_mistakes.append(line[:-1])\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for item in total_mistakes]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225db5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs_csc(file_name):\n",
    "    logging.info((\"Reading lines from {}\".format(file_name)))\n",
    "    total_correct=[]\n",
    "    total_mistakes=[]\n",
    "    with codecs.open(file_name, \"r\", \"utf-8\") as file:\n",
    " \n",
    "        data = file.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        results = soup.find_all('essay')\n",
    "        for item in tqdm(results):\n",
    "            correct_sentences = []\n",
    "            wrong_sentences = []\n",
    "            for passage in item.find_all('passage'):\n",
    "                text = passage.text\n",
    "                mistake = item.find('mistake', {'id': passage['id']})\n",
    "                if mistake:\n",
    "                    wrong_text = mistake.wrong.text\n",
    "                    correct_text = mistake.correction.text\n",
    "                    text = text.replace(wrong_text, correct_text)\n",
    "                    wrong_sentences.append(passage.text)\n",
    "                    correct_sentences.append(text)\n",
    "            total_correct=total_correct+correct_sentences\n",
    "            total_mistakes=total_mistakes+wrong_sentences\n",
    "    assert len(total_correct)==len(total_mistakes)\n",
    "    return total_correct,total_mistakes\n",
    "def rewrite_csc(file_name):\n",
    "    total_correct,total_mistakes=read_langs_csc(file_name+\".sgml\")\n",
    "    with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "        fp.close()\n",
    "    with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d6225d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 3166.62it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./clp14csc/Training/C1_training'\n",
    "rewrite_csc(file_name)\n",
    "# file_name='./clp14csc/Training/B1_training'\n",
    "# rewrite_csc(file_name)\n",
    "# 存在编码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83cf8bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 442/442 [00:00<00:00, 5848.62it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name='./sighan8csc/Training/SIGHAN15_CSC_A2_Training'\n",
    "rewrite_csc(file_name)\n",
    "# file_name='./sighan8csc/Training/SIGHAN15_CSC_B2_Training'\n",
    "# rewrite_csc(file_name)\n",
    "# 存在编码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97bebfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs_hybrid(file_name):\n",
    "    total_mistakes=[]\n",
    "    total_correct=[]\n",
    "    for line in open(file_name+\"_1.src\",'r',encoding='utf8'): \n",
    "        total_mistakes.append(line[:-1].split('\\t')[1])\n",
    "    for line in open(file_name+\"_1.trg\",'r',encoding='utf8'): \n",
    "        total_correct.append(line[:-1].split('\\t')[1])\n",
    "    assert len(total_correct)==len(total_mistakes)\n",
    "    return total_correct,total_mistakes\n",
    "def rewrite_hybrid(file_name):\n",
    "    total_correct,total_mistakes=read_langs_hybrid(file_name)\n",
    "    with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "        fp.close()\n",
    "    with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89fc78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='./hybrid/hybird_train'\n",
    "rewrite_hybrid(file_name)\n",
    "file_name='./hybrid/hybird_test'\n",
    "rewrite_hybrid(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa8b1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./sighan15/sighan.train.ccl22'\n",
    "for line in open(file_name+\".para\",'r',encoding='utf8'): \n",
    "    data=line[:-1].split(' ||| ')\n",
    "    total_mistakes.append(data[0])\n",
    "    total_correct.append(data[1])\n",
    "    \n",
    "\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7509a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "file_name='./WANG27/wang.train.ccl22'\n",
    "for line in open(file_name+\".para\",'r',encoding='utf8'): \n",
    "    data=line[:-1].split(' ||| ')\n",
    "    total_mistakes.append(data[0])\n",
    "    total_correct.append(data[1])\n",
    "    \n",
    "\n",
    "assert len(total_correct)==len(total_mistakes)\n",
    "with open(file_name+\".src\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_mistakes]\n",
    "    fp.close()\n",
    "with open(file_name+\".trg\",'w', encoding=\"utf-8\") as fp:\n",
    "    [fp.write(str(item)+'\\n') for  item in total_correct]\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c21441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=['./CGED/cged2014/nlptea14cfl_release1.1/Training/A2_CFL_training','./CGED/cged2014/nlptea14cfl_release1.1/Training/B2_CFL_training','./CGED/cged2014/nlptea14cfl_release1.1/Training/C1_CFL_training','./CGED/cged2015/nlptea15cged_release1.0/Training/NLPTEA15_CGED_Training','./CGED/cged2016/nlptea16cged_release1.0/Training/CGED16_HSK_TrainingSet','./CGED/cged2017/train.release','./CGED/cged2018/train_CGED2018','./CGED/cged2020/test_truth_2020mk','./CGED/cged2021/test_2021','./CTC2021/train/train_large_v2','./HSK/hsk','./Lang8/data.train','./MuCGEC/MuCGEC_dev','./MuCGEC_exp_data/train/train','./MuCGEC_exp_data/valid/valid','./YACLC/valid']\n",
    "file_names.append('./clp14csc/Training/C1_training')\n",
    "file_names.append('./sighan8csc/Training/SIGHAN15_CSC_A2_Training')\n",
    "file_names.append('./hybrid/hybird_train')\n",
    "file_names.append('./hybrid/hybird_test')\n",
    "file_names.append('./sighan15/sighan.train.ccl22')\n",
    "file_names.append('./WANG27/wang.train.ccl22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e61ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_1(file_name):\n",
    "    srcs=[]\n",
    "    trgs=[]\n",
    "    for line in open(file_name+\".src\",'r',encoding='utf8'): \n",
    "        srcs.append(line[:-1])\n",
    "    for line in open(file_name+\".trg\",'r',encoding='utf8'): \n",
    "        trgs.append(line[:-1])\n",
    "    res={}\n",
    "    for src,trg in zip(srcs,trgs):\n",
    "        if src not in res:\n",
    "            res[src]=[trg]\n",
    "        else:\n",
    "            res[src].append(trg)\n",
    "    with open(file_name+\"_1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        i = 0\n",
    "        for key, items in res.items():\n",
    "            line = str(i) + \"\\t\" + key + \"\\t\" + \"\\t\".join(items) + \"\\n\"\n",
    "            file.write(line)\n",
    "            i=i+1\n",
    "\n",
    "for file_name in file_names:\n",
    "    convert_1(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05b77f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_2(file_name):\n",
    "    srcs=[]\n",
    "    trgs=[]\n",
    "    task=file_name.split('/')[1]\n",
    "    name=file_name.split('/')[-1]\n",
    "    for line in open(file_name+\".src\",'r',encoding='utf8'): \n",
    "        srcs.append(line[:-1])\n",
    "    for line in open(file_name+\".trg\",'r',encoding='utf8'): \n",
    "        trgs.append(line[:-1])\n",
    "    \n",
    "    with open(file_name+\"_1.src\", \"w\", encoding=\"utf-8\") as file:\n",
    "        i = 0\n",
    "        for item in srcs:\n",
    "            line = task+\"_\"+name+\"_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "            file.write(line)\n",
    "            i=i+1\n",
    "    with open(file_name+\"_1.trg\", \"w\", encoding=\"utf-8\") as file:\n",
    "        i = 0\n",
    "        for item in trgs:\n",
    "            line = task+\"_\"+name+\"_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "            file.write(line)\n",
    "            i=i+1\n",
    "\n",
    "for file_name in file_names:\n",
    "    convert_2(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9df2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='./CSC_val/val'\n",
    "convert_1(file_name)\n",
    "convert_2(file_name)\n",
    "file_name='./CSC_test/test'\n",
    "srcs=[]\n",
    "for line in open(file_name+\".src\",'r',encoding='utf8'): \n",
    "    srcs.append(line[:-1])\n",
    "with open(file_name+\"_1.src\", \"w\", encoding=\"utf-8\") as file:\n",
    "    i = 0\n",
    "    for item in srcs:\n",
    "        line = \"CSC_test_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "        file.write(line)\n",
    "        i=i+1\n",
    "with open(file_name+\".txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for item in srcs:\n",
    "        line = item + \"\\n\"\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c8fd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_1(file_names):\n",
    "    srcs=[]\n",
    "    trgs=[]\n",
    "    for file_name in file_names:\n",
    "        for line in open(file_name+\".src\",'r',encoding='utf8'): \n",
    "            srcs.append(line[:-1])\n",
    "        for line in open(file_name+\".trg\",'r',encoding='utf8'): \n",
    "            trgs.append(line[:-1])\n",
    "    res={}\n",
    "    for src,trg in zip(srcs,trgs):\n",
    "        if src not in res:\n",
    "            res[src]=[trg]\n",
    "        else:\n",
    "            res[src].append(trg)\n",
    "    with open(\"./CSC_train/train_1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        i = 0\n",
    "        for key, items in res.items():\n",
    "            line = str(i) + \"\\t\" + key + \"\\t\" + \"\\t\".join(items) + \"\\n\"\n",
    "            file.write(line)\n",
    "            i=i+1\n",
    "\n",
    "convert_all_1(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e834fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_2(file_names):\n",
    "    srcs=[]\n",
    "    trgs=[]\n",
    "    for file_name in file_names:\n",
    "        for line in open(file_name+\"_1.src\",'r',encoding='utf8'): \n",
    "            srcs.append(line[:-1])\n",
    "        for line in open(file_name+\"_1.trg\",'r',encoding='utf8'): \n",
    "            trgs.append(line[:-1])\n",
    "        if len(srcs)!=len(trgs):\n",
    "            print(file_name)\n",
    "    with open(\"./CSC_train/train_1.src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in srcs]\n",
    "        fp.close()\n",
    "    with open(\"./CSC_train/train_1.trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in trgs]\n",
    "        fp.close()\n",
    "\n",
    "convert_all_2(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43628e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_3(file_names):\n",
    "    srcs=[]\n",
    "    trgs=[]\n",
    "    for file_name in file_names:\n",
    "        for line in open(file_name+\".src\",'r',encoding='utf8'): \n",
    "            srcs.append(line[:-1])\n",
    "        for line in open(file_name+\".trg\",'r',encoding='utf8'): \n",
    "            trgs.append(line[:-1])\n",
    "    with open(\"./CSC_train/train.src\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in srcs]\n",
    "        fp.close()\n",
    "    with open(\"./CSC_train/train.trg\",'w', encoding=\"utf-8\") as fp:\n",
    "        [fp.write(str(item)+'\\n') for  item in trgs]\n",
    "        fp.close()\n",
    "\n",
    "convert_all_3(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1bc7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhconv import convert\n",
    "\n",
    "def E_trans_to_C(string):\n",
    "    E_pun = u',!?[]()<>\"\\';:'\n",
    "    C_pun = u'，！？【】（）《》“‘；：'\n",
    "    table= {ord(f):ord(t) for f,t in zip(E_pun,C_pun)}\n",
    "    return string.translate(table)\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert text from traditional to simplified Chinese\n",
    "    text = convert(text, 'zh-cn')\n",
    "    \n",
    "    # Normalize punctuation\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = E_trans_to_C(text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\u4e00-\\u9fa5，。？！；：“”‘’《》【】（）〔〕…—-]\", '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8713c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "pairs6=[]\n",
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "for line in open(\"CSC_train/train_1.src\",'r',encoding='utf8'): \n",
    "    total_mistakes.append(normalize_text(line[:-1].split(\"\\t\")[1]))\n",
    "for line in open(\"CSC_train/train_1.trg\",'r',encoding='utf8'): \n",
    "    total_correct.append(normalize_text(line[:-1].split(\"\\t\")[1]))\n",
    "pairs=[]\n",
    "for srcs, trgs in zip(total_mistakes,total_correct):\n",
    "    srcs2=re.sub(r'[^\\u4e00-\\u9fa5]', '', srcs)\n",
    "    trgs2=re.sub(r'[^\\u4e00-\\u9fa5]', '', trgs)\n",
    "    if len(srcs)==len(trgs) and len(trgs)!=0 and len(srcs2)==len(trgs2) and len(trgs2)!=0:\n",
    "        pairs.append((srcs, trgs))\n",
    "    else:\n",
    "        pairs6.append((srcs, trgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55c7233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def is_same_elements(list1, list2):\n",
    "    return Counter(list1) == Counter(list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e14669c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223757 481665 204466 61297 20812 8238 2491930\n"
     ]
    }
   ],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "for item in pairs:\n",
    "    total_mistakes.append(item[0])\n",
    "    total_correct.append(item[1])\n",
    "pairs0=[]\n",
    "pairs1=[]\n",
    "pairs2=[]\n",
    "pairs3=[]\n",
    "pairs4=[]\n",
    "pairs5=[]\n",
    "#pairs6=[]\n",
    "for srcs, trgs in zip(total_mistakes,total_correct):\n",
    "    i=0\n",
    "    for index, (src, trg) in enumerate(zip(srcs, trgs)):\n",
    "        if src != trg:\n",
    "            i=i+1\n",
    "    if is_same_elements(srcs, trgs) and srcs!=trgs:\n",
    "        pairs6.append((srcs, trgs))\n",
    "        continue\n",
    "    if i>int(len(srcs)/5):\n",
    "        pairs6.append((srcs, trgs))\n",
    "        continue\n",
    "    if i==0:\n",
    "        pairs0.append((srcs, trgs))\n",
    "    if i==1:\n",
    "        pairs1.append((srcs, trgs))\n",
    "    if i==2:\n",
    "        pairs2.append((srcs, trgs))\n",
    "    if i==3:\n",
    "        pairs3.append((srcs, trgs))\n",
    "    if i==4:\n",
    "        pairs4.append((srcs, trgs))\n",
    "    if i==5:\n",
    "        pairs5.append((srcs, trgs))\n",
    "    if i>5:\n",
    "        pairs6.append((srcs, trgs))\n",
    "print(len(pairs0),len(pairs1),len(pairs2),len(pairs3),len(pairs4),len(pairs5),len(pairs6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "307a879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=pairs0+pairs1+pairs2+pairs3+pairs4+pairs5\n",
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "for item in pairs:\n",
    "    total_mistakes.append(item[0])\n",
    "    total_correct.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79bbe9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000235"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "429484a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CSC_train/train_no_Aug.src\", \"w\", encoding=\"utf-8\") as file:\n",
    "    i = 0\n",
    "    for item in total_mistakes:\n",
    "        line = \"train_CSC_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "        file.write(line)\n",
    "        i=i+1\n",
    "with open(\"CSC_train/train_no_Aug.trg\", \"w\", encoding=\"utf-8\") as file:\n",
    "    i = 0\n",
    "    for item in total_correct:\n",
    "        line = \"train_CSC_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "        file.write(line)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca7b50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt=set()\n",
    "for item in pairs6:\n",
    "    tgt.add(item[1])\n",
    "tgt.remove(\"\")\n",
    "with open(\"CSC_train/train_Aug.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    i = 0\n",
    "    for item in tgt:\n",
    "        line = \"train_CSC_Aug_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "        file.write(line)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6cdb932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192788"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f8757aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=[]\n",
    "total_correct=[]\n",
    "for line in open(\"./CSC_train/augmentation.csv\",'r',encoding='utf8'): \n",
    "    lines=line[:-1].split(\"\\t\")\n",
    "    if len(lines)!=3:\n",
    "        continue\n",
    "    total_mistakes.append(lines[2])\n",
    "    total_correct.append(lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76c4d098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490685"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec69137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mistakes=total_mistakes[1:]\n",
    "total_correct=total_correct[1:]\n",
    "for line in open(\"./CSC_train/train_no_Aug.src\",'r',encoding='utf8'): \n",
    "    total_mistakes.append(line[:-1].split(\"\\t\")[1])\n",
    "for line in open(\"./CSC_train/train_no_Aug.trg\",'r',encoding='utf8'): \n",
    "    total_correct.append(line[:-1].split(\"\\t\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc190a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490919"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e9c3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224018 721007 389507 101539 36082 13735 5031\n"
     ]
    }
   ],
   "source": [
    "pairs=list(zip(total_mistakes,total_correct))\n",
    "import random\n",
    "random.shuffle(pairs)\n",
    "srcs1,trgs1=zip(*pairs)\n",
    "from collections import Counter\n",
    "\n",
    "def is_same_elements(list1, list2):\n",
    "    return Counter(list1) == Counter(list2)\n",
    "\n",
    "pairs0=[]\n",
    "pairs1=[]\n",
    "pairs2=[]\n",
    "pairs3=[]\n",
    "pairs4=[]\n",
    "pairs5=[]\n",
    "pairs6=[]\n",
    "for srcs, trgs in zip(srcs1,trgs1):\n",
    "    i=0\n",
    "    for index, (src, trg) in enumerate(zip(srcs, trgs)):\n",
    "        if src != trg:\n",
    "            i=i+1\n",
    "    if is_same_elements(srcs, trgs) and srcs!=trgs:\n",
    "        pairs6.append((srcs, trgs))\n",
    "        continue\n",
    "    if i>int(len(srcs)/5):\n",
    "        pairs6.append((srcs, trgs))\n",
    "        continue\n",
    "    if i==0:\n",
    "        pairs0.append((srcs, trgs))\n",
    "    if i==1:\n",
    "        pairs1.append((srcs, trgs))\n",
    "    if i==2:\n",
    "        pairs2.append((srcs, trgs))\n",
    "    if i==3:\n",
    "        pairs3.append((srcs, trgs))\n",
    "    if i==4:\n",
    "        pairs4.append((srcs, trgs))\n",
    "    if i==5:\n",
    "        pairs5.append((srcs, trgs))\n",
    "    if i>5:\n",
    "        pairs6.append((srcs, trgs))\n",
    "print(len(pairs0),len(pairs1),len(pairs2),len(pairs3),len(pairs4),len(pairs5),len(pairs6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2aed855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./CSC_train/train_Aug.src\", \"w\", encoding=\"utf-8\") as file:\n",
    "    i = 0\n",
    "    for item in srcs1:\n",
    "        line = \"train_Aug_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "        file.write(line)\n",
    "        i=i+1\n",
    "with open(\"./CSC_train/train_Aug.trg\", \"w\", encoding=\"utf-8\") as file:\n",
    "    i = 0\n",
    "    for item in trgs1:\n",
    "        line = \"train_Aug_\"+str(i) + \"\\t\" + item + \"\\n\"\n",
    "        file.write(line)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e235c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
